import numpy as np

class fullconnectedlayer(object):
    def __init__(self, dim, N):
        """
        --N number of neurons in the full connected layer
        --dim Number of dimention of input data
        """
        self.weights = np.random.randn(dim, N)/np.sqrt(dim)
        self.bias = np.random.randn(1, N)
        
    def fw(self, x):
        """
        --x input of the neurol R = [batch_size, dim]
        """
        return np.dot(x, self.weights) + self.bias

    def bw(self, dscore, a_input, lr):
        """
        dscore --dscore from layer after this layer
        a_input --input activition from layer before this layer
        """
        dw = np.dot(a_input.T, dscore)
        db = np.sum(dscore, axis = 0, keepdims = True)
        dscore = np.dot(dscore, self.weights.T)
        self.weights -= lr * dw
        self.bias -= lr * db
        return dscore, dw, db

class fullconnectedrelulayer(fullconnectedlayer):
    def fw(self, x):
        """
        --x input of the neurol R = [batch_size, dim]
        """
        self.a = np.maximum(0, np.dot(x, self.weights) + self.bias)
        return self.a
    
    def bw(self, dscore, a_input, lr):
        """
        dscore --dscore from layer after this layer
        a_input --input activition from layer before this layer
        """
        dscore[self.a <= 0] = 0
        dw = np.dot(a_input.T, dscore)
        db = np.sum(dscore, axis = 0, keepdims = True)
        dscore = np.dot(dscore, self.weights.T)
        self.weights -= lr * dw
        self.bias -= lr * db
        return dscore, dw, db

class relu(object):
    def fw(x):
        return np.maximum(0, x)
    
    def bw(x):
        return np.maximum(0, x)
    
class softmax(object):        
    def fw(self, a):
        return np.exp(a)/np.sum(np.exp(a), axis = 1, keepdims = True)


class CrossEntropyCost(object):
    @staticmethod
    def fn(a, y):
        """Return the cost associated with an output ``a`` and desired output
        ``y``.  Note that np.nan_to_num is used to ensure numerical
        stability.  In particular, if both ``a`` and ``y`` have a 1.0
        in the same slot, then the expression (1-y)*np.log(1-a)
        returns nan.  The np.nan_to_num ensures that that is converted
        to the correct value (0.0).

        """
        y_mat = np.zeros(a.shape)
        y_mat[np.arange(0, a.shape[0]), y] = 1
#       loss = -np.sum((y_mat*np.log(OP)) + (1- y_mat)*np.log(1 - OP))/100
        loss = -np.sum(np.nan_to_num(y_mat*np.log(a))) / a.shape[0]
        return loss
    
    @staticmethod
    def delta(a, y):
        """Return the error delta from the output layer.  Note that the
        parameter ``z`` is not used by the method.  It is included in
        the method's parameters in order to make the interface
        consistent with the delta method for other cost classes.

        """
        y_mat = np.zeros(a.shape)
        y_mat[np.arange(0, a.shape[0]), y] = 1
        return (a-y_mat)

"""
Three layer network
"""
N = 100 # number of points per class
D = 2 # dimensionality
K = 3 # number of classes
X = np.zeros((N*K,D)) # data matrix (each row = single example)
y = np.zeros(N*K, dtype='uint8') # class labels
for j in range(K):
  ix = range(N*j,N*(j+1))
  r = np.linspace(0.0,1,N) # radius
  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta
  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j
# lets visualize the data:
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.show()


lr = 1e-3



lyr1 = fullconnectedrelulayer(2, 100)
lyr2 = fullconnectedrelulayer(100, 100)
lyr3 = fullconnectedlayer(100, 3)
softmaxlyr = softmax()
lossfun = CrossEntropyCost()

for i in range(50):
    #forward pass
    a1 = lyr1.fw(X)
    a2 = lyr2.fw(a1)
    a3 = lyr3.fw(a2)
    a4 = softmaxlyr.fw(a3)
    loss = lossfun.fn(a4, y)
    print(loss)

    #backward pass
    dscore = lossfun.delta(a4, y)
    dscore /= num_example
    dscore, dw3, db3 = lyr3.bw(dscore, a2, lr)
    dscore, dw2, db2 = lyr2.bw(dscore, a1, lr)
    dscore, dw1, db1 = lyr1.bw(dscore, X, lr)
